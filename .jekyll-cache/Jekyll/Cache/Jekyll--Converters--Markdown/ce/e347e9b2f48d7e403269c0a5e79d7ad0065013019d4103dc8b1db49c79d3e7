I"¶<h2 id="abstract">Abstract</h2>
<p>Data-driven machine learning paradigms now solve some of the worldâ€™s hardest problems by learning from data. Unfortunately, what is learned is often unknown to both the people who train models and the people they impact. My research addresses these challenges by enabling machine learning interpretability at scale and for everyone, through designing and developing interactive interfaces that help people confidently understand data-driven systems.</p>

<p>(1) Operationalizing interpretability: My Gamut and TeleGam systems operationalize interpretability through design probes that investigate the emerging practice of interpretability. Gamut has been deployed at Microsoft Research and demoed for executive leadership.</p>

<p>(2) Scaling up interpretability: My Summit system scales interpretability to large-scale neural networks and datasets, for example ImageNet with 1.3M+ images, by summarizing and visualizing what features a deep learning model has learned and how those features interact to make predictions.</p>

<p>(3) Communicating interpretability: Through the new medium of interactive articles, my work accelerates research dissemination and broadens peopleâ€™s education access to modern AI technologies. With the Parametric Press, a new interactive publishing platform I co-launched, my work has helped over 250,000+ people learn about machine learningâ€™s capabilities and modern applications.</p>

<p>My interdisciplinary research contributes to human-computer interaction, machine learning, and more importantly their intersection. Through collaborating closely with researchers, designers, and practitioners, my research is making an impact in academia, industry, government, and society, including open-source interactive systems, scalable algorithms, and widely accessible communication artifacts.</p>

<h2 id="slides">Slides</h2>
<ul>
  <li>PDF: <a href="https://www.dropbox.com/s/q67wq3eedr88yey/talk-low-quality.pdf?dl=0">Dropbox, low quality (50MB)</a></li>
  <li>PDF: <a href="https://www.dropbox.com/s/crmfc8gusn9rtt4/talk-high-quality.pdf?dl=0">Dropbox, high quality (200MB)</a></li>
  <li>Movie export with animations + demo videos:</li>
</ul>

<p>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/UfJoqQGXIGc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>

<h2 id="materials">Materials</h2>
<ul>
  <li>Research Statement (PDF): <a href="http://localhost:4000/research-statement.pdf">fredhohman.com/research-statement.pdf</a></li>
  <li>CV (Web): <a href="https://fredhohman.com/cv">fredhohman.com/cv</a></li>
  <li>CV (PDF): <a href="https://fredhohman.com/cv.pdf">fredhohman.com/cv.pdf</a></li>
</ul>

<h2 id="bio">Bio</h2>
<p>Fred Hohman is a PhD candidate at Georgia Techâ€™s College of Computing.</p>

<p>His research focuses on enabling machine learning interpretability at scale and for everyone, by designing and developing interactive interfaces to help people confidently understand data-driven systems. Besides building tools, he also creates data visualizations and writes interactive articles to simply communicate complex ideas.</p>

<p>He has collaborated with designers, developers, and scientists at Apple, Microsoft Research, NASA JPL Human Interfaces, and Pacific Northwest National Lab. He won a NASA PhD Space Technology Research Fellowship, a Microsoft AI for Earth Award for using AI to improve sustainability, and the Presidentâ€™s Fellowship for top incoming PhD students. He has also won an ACM CHI 2019 Best Paper award; a KDD 2018 Audience Appreciation Award, Runner-up; an IEEE VIS VISxAI Best Paper, Honorable Mention; and a SIGMOD 2017 Best Demo, Honorable Mention. His work has appeared in popular press, such as the Stack Overflow Blog, Fast Company, and Data Stories. He co-organizes the Workshop on Visualization for AI Explainability (VISxAI) at IEEE VIS. He double majored in mathematics and physics.</p>

:ET